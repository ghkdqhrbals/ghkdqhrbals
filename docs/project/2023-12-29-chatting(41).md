---
layout: default
title: 41. 서버성능 개선기록 - 🟢TPS 31%, MTTFB[p99] 39% 개선
parent: 📌 실시간 채팅서버 프로젝트
nav_order: 41
---

created at 2023-12-29
{: .label .label-yellow }

## 1. 개선 이전
<img width="1412" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/d2c0eff9-8bbc-44db-a24e-4e36a27acf35">

thread 300 으로 요청 전송 시 위와같이 많은 cpu 로드가 걸리네요. cpu limit 설정은 적용되어 있지 않아 보이지만 일반적으로 파드 하나로 돌리기에는 많은 부하를 혼자 감당하고 있는것을 확인할 수 있었습니다.

## 2. ingress 파드 replicaSet=2 CPU 로드 분산 시 각 파드 CPU 리소스 소모 관찰

<img width="1001" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/78afa556-691e-421a-9f68-2bf7f1f4f4a0">

<img width="1002" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/3d5041fd-8faa-420e-b507-c1c6b1ada47f">


AWS **NLB** 에서 각 pods 로 로드밸런싱을 수행하게 되고, 그 결과로 CPU Usage 또한 분산될 수 있었습니다!

## 3. Ingress Pod 가 **2**개일 때 성능지표 측정

> * 🟢 : 성능향상
> * 🔴 : 성능감소

| Metric | Before | After | Change |
|--------|--------|-------|--------|
| TPS 평균 | 319.99 | 422.20 | **31.94% 🟢** |
| TPS p95 | 376.77 | 497.80 | **32.12% 🟢** |
| TPS p99 | 415.61 | 532.80 | **28.20% 🟢** |
| MTTFB 평균 | 950.89 ms | 709.86 ms | **25.35% 🟢** |
| MTTFB p95 | 1322.11 ms | 958.64 ms | **27.49% 🟢** |
| MTTFB p99 | 1833.22 ms | 1117.45 ms | **39.04% 🟢** |
| MTTFB 차이 평균 | 112.52 ms | 58.82 ms | **47.72% 🟢** |
| MTTFB 평균적인 변동률 | 10.67% | 7.67% | **28.12% 🟢** |

정말 큰 폭으로 TPS 와 MTTFB 지표가 변한것을 확인할 수 있습니다. CPU Usage 는 각 워커노드로 잘 분산된 것 또한 이전 그래프를 통해 관찰할 수 잇었죠! 파드가 2개로 늘어났을 때 이정도이면 단순히 생각했을 때 3개의 Ingress Pod 라면 어떨까요?

## 4. Ingress Pod 가 **3**개일 때 성능지표 측정

| Metric | Before | After | Change |
|--------|--------|-------|--------|
| TPS 평균 | 422.20 | 462.79 | **9.61% 🟢** |
| TPS p95 | 497.80 | 556.60 | **11.81% 🟢** |
| TPS p99 | 532.80 | 588.62 | **10.48% 🟢** |
| MTTFB 평균 | 709.86 ms | 651.05 ms | **8.28% 🟢** |
| MTTFB p95 | 958.64 ms | 1017.44 ms | **-6.13% 🔴** |
| MTTFB p99 | 1117.45 ms | 1265.16 ms | **-13.22% 🔴** |
| MTTFB 차이 평균 | 58.82 ms | 88.51 ms | **-50.48% 🔴** |
| MTTFB 평균적인 변동률  | 7.67% | 11.53% | **-50.33% 🔴** |

TPS, MTTFB mean 값이 소폭 상승한 것을 확인할 수 있었습니다. **하지만 최악의 시나이로를 가정할 수 있는 p95, p99 측정은 MTTFB 가 조금 낮아진 것을 확인할 수 있었습니다.** 조금 이상하죠? 보통 TPS 와 MTTFB 는 비례관계라고 생각하는데 위의 결과는 반대인 부분이 이상합니다. 이유가 뭘까요? 차근차근 분석해볼게요. 본 분석은 17:05분, 17:45분 시작된 10분동안의 서버 로드테스트의 각 지표를 확인할 것입니다.

1. HTTP Request Response 확인

<img width="1410" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/73185a6e-e11e-40cc-983e-aa949ce6ee47">

Request 수의 차이는 없군요! 하지만 응답시간이 갑자기 확 증가한 부분이 보이죠? 이 시점에서의 다른 지표들을 관찰해볼게요.

2. 메모리 GC 에 소요된 시간 및 GC count

여기서 우리는 minor GC 의 빈도가 확 높아졌고 latency 또한 증가하는 것을 확인할 수 있습니다. 뿐만 아니라 majorGC 도 발생했습니다. 왜이렇게 증가했을까요?

<img width="1412" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/7a6d18c8-be9d-4147-ae4b-ff2ac00bcf93">

## 5. Heap GC 로 인한 Stop The World 지연시간 확인

minor GC 가 테스트 동안 50번 정도 발생했으며, 지연시간은 약 8ms 로 아직 과하지 않은 힙 사용량을 확인할 수 있었습니다.

<img width="1416" alt="image" src="https://github.com/ghkdqhrbals/spring-chatting-server/assets/29156882/943c1214-3d93-4415-9908-02e40a46ebc8">

