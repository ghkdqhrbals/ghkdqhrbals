---
layout: default
title: 📌 XAI를 활용한 취약점 분석 웹 어플리케이션
nav_order: 2
has_children: true
---

# **[@FOXEE]**

본 문서는 스타트업 FOXEE 의 **XAI를 활용한 취약점 분석 웹 어플리케이션** 개발 과정에서 발생하는 여러가지 통찰점을 개인적으로 기록한 문서입니다 😊

### 1. 현재 까지의 아키텍처 설계 및 구현 사항들

![img.png](../arc1.png)

### 2. 각 tag 에 대한 상세설명
**(a)** 모델서버는 Kafka MQ 에 아래의 구조로 이벤트를 전송합니다.

* userId, eventId, serverId, analyzeId, status

메인서버는 모델서버로부터 실시간으로 분석정보와 진행상황을 받아야합니다.

이 때 쉽지않았던 부분은 실시간 통신에 항상 열려있는 websocket 이나 SSE 가 필요했습니다. 기본적으로 3분에서 10분동안 모델서버가 간간히 분석결과를 전송하게 되는데, 이 긴 시간동안 네트워크 유실 위험성도 존재하며 File Descriptor(각 websocket 이나 SSE 마다 사용되는 I/O 식별자)의 자원을 많이 먹는다는 문제 또한 존재했습니다(또한 각 소켓 별 하나의 스레드로 listening 해야하기에 스레드가 부족해집니다, 물론 하나의 스레드로 여러 I/O 를 관리하는 리액티브 기술은 이 문제를 일시적으로 해결할 수 있습니다).

{: .note-title }
> File Descriptor 란?
>
> 파일 디스크립터(File Descriptor)는 운영 체제가 열린 파일 또는 소켓과 관련된 I/O 작업을 추적하는 데 사용하는 식별자입니다. 일반적으로 리눅스가 꽤 높은 FD 개수를 가지고 있습니다.


따라서 네트워크 유실 시 장애 복구와 불필요한 자원소모를 줄이기 위해서, 이벤트를 안전하게 파일 시스템에 저장하고 offset 으로 장애복구가 가능하며 적은 스레드로 간간히 전송하는 이벤트들을 받을 수 있는 Kafka 를 사용하기로 하였습니다.
{: .important-title }
> 왜 Kafka 를 사용했을까요?
>
> 물론 하나의 웹 소켓에 여러 서버가 동시에 데이터를 전송하는것은 가능합니다. 이렇게 하면 효과적으로 자원을 소모시킬 수 있죠. 다만, 웹 소켓을 클러스터링하여 관리해야하는 수고가 존재하며 네트워크 유실시 장애대응이 불가능합니다. 1. 서버측의 연결을 재확인해야하며, 2. 서버 간 서로의 세션을 관리해야하겠죠. 3. 여러대의 서버를 자동으로 묶어주는 클러스터링도 필요할 것이며, 4. 서버 간 메세지 읽고 쓰는 속도를 조절하기 위해 백프래셔 또한 존재해야할 것입니다. 이 밖에 신경쓸 부분들이 너무나도 많기때문에 이미 대부분 구현되어 있고 안전성이 검증된 Kafka 를 사용한 것입니다.

또한 Kafka 에 TLS 설정 이나 ACL(Access Controll List) 설정을 따로 할 필요가 없었습니다. 이유는 대부분의 서버가 내부 네트워크망 에서 돌아가며 Gateway 와 EUREKA 서버만 노출되어있기때문이죠.

**(b)** 모든 Gateway 는 EUREKA server 에 등록된 서버들에게 로드밸런싱을 수행합니다

EUREKA 서버를 통해서 모든 서버들의 url 를 한곳에서 관리하고 이를 Gateway 가 자동으로 읽고 로드밸런싱하도록 만들었습니다. 일종의 개발 편리성을 위해 만든 서버죠. 그리고 메인 서버와 모델 서버들이

**(c)** Redis 에 이벤트를 모두 저장합니다. (10 min 초과 시, 자동삭제)

* userId, eventId, serverId, analyzeId, status, createdAt

저는 Kafka 로 부터 수신한 이벤트를 저장해야합니다. 최종 이벤트 수신 시, 이전 같은 event id 를 가지는 이벤트들과 비교하는 로직을 수행해야하기 때문입니다. 그러기 위해서는 어디에 저장하는 지가 중요했습니다. 이 이벤트 저장은 빈번하게 일어나며, 이벤트 저장 시 트랜젝션 격리는 필요없습니다. 빠르게 이벤트들은 저장되어야하며, 빠르게 읽을 수 있어야 합니다. 이를 위해서 저는 Redis 인메모리 DB를 선택하였습니다. 메모리 위에 저장하기때문에 상당히 빠른 성능을 보여줍니다. 데이터 최대 유지 시간은 모든 서비스를 통틀어 걸리는 최대 소요 시간인 10분을 잡았습니다.

**(d)** WebSocket 으로 실시간 분석진행도를 전달합니다.

이벤트를 KafkaListener 을 통해서 읽고, Redis 에 저장과 동시에 사용자에게 동일한 json 을 소켓을 통해 전송합니다. 소켓을 통해 클라이언트가 분석 중지를 요청하면, rollback 을 수행합니다.

**(e)** 메인 서버는 모델B 서버에게 파일과 함께 분석 API 를 요청합니다. 그리고 모델 서버는 (a) 를 지속적으로 수행합니다.

이 진행상황 이벤트는 멱등성이 중요한 로직은 아니지만, 이 후 멱등성을 보장할 수도 있기 때문에 하나의 메세지를 Eventually consistance 하게 성공시키는 Exactly-Once 형식으로  전송하게됩니다.

**(f)** 모델 서버에 GET /analysis?serverId=143 을 전송하면, Gateway-3 은 EUREKA 서버에게 MODEL_SERVER 의 url 들을 요청합니다. 또한 해당 서버들의 메타데이터를 요청합니다. 이후 메타데이터에 저장된 serverId 가 143인 서버 url 에게 라우팅합니다.

여기서 serverId 는 자동으로 로드밸런싱된 파일저장 API 가 도착한 모델 서버의 고유번호입니다. 즉, 파일 저장위치를 나타내는 서버식별번호인것이죠. 모델 서버는 유레카 서버에 자신을 등록할 때 메타데이터에 고유번호와 함께 등록시킵니다.

모델 서버가 수평적으로 스케일링 된다는 가정 때문에 서버 식별번호를 부여할 수 밖에 없었습니다. 만약 그렇지 않으면, 분석이 끝난 모델 서버가 저장하고있는 분석결과 파일의 실제 위치가 Model B 중 어디인지 모르기 때문이죠.

**(g)** **(f)** 에서 요청한 결과들을 압축하여 스트림형식으로 받아와서 메인 서버에 저장합니다. 저장이 끝나면 메인 서버는 모델 서버에게 분석결과들을 삭제를 지시합니다. 이미 최종 결과물들은 메인서버에 저장이 되었기 때문이죠.

**(h)** TLS 오프로딩은 서버보다 앞에서 중간 매개체(여기서는 리버스 프록시인 Gateway)가 TLS 암호화/복호화를 수행하는 기술입니다. 서버가 프록시와 내부망에 함께 있어야 안전합니다. 현재의 경우에는 함께 있지만 추후 메인서비스가 떨어지게 된다면, 하나의 TLS 를 더  추가해야됩니다.

### 3. 여러 제약사항들

* 모델서버의 메모리는 16GB 로 상당히 작습니다.
> 기본적으로 하나의 분석 API 요청은 약 1GB 정도의 파일과 함께 요청되기때문에 빠르게 메모리에서 꺼내야합니다. 파일을 chunk 단위로 읽는 바이트 스트림형태로 그때그때 읽어서 처리해야합니다.
* 분석할 파일의 용량이 1GB 로 큰 편입니다.
> 현재 서버의 네트워크 대역폭은 1Gbps 으로 매우 작습니다. 따라서 동시에 여러개의 파일을 수신하기 어렵습니다. 이는 위의 제약사항과 직결됩니다. 마찬가지로 파일을 스트림으로 읽어 빠르게 처리하도록 변경해야했습니다.
* 인력이 부족합니다.
> 백엔드 전체를 저 혼자서 맡다보니 TDD 도 무리가 있으며, 고민해봐야할 문제들이 넘쳐났습니다. 따라서 현실적으로 보았을 때 인증서버와 메인서비스 로직을 하나의 서버로 묶고, 모델서버와 게이트웨이를 하나의 서버로 묶는 방식으로 개발을 진행하였습니다. 이렇게 하면 인력이 부족한 상황에서도 개발을 진행할 수 있었습니다.
* 모델서버는 수평 확장가능해야합니다.
> 같은 모델이나 다른 모델을 수평적으로 확장가능해야했습니다. 그렇다면 분석할 파일을 어디에 전송할 지 애매합니다. 모델서버 부하분산 시, 분석파일이 전송되는 서버의 실제 위치를 알아야합니다. 그래야지만 나중에 분석이 실패했을 때, 어디에 저장된 파일을 삭제해야할 지 알 수 있기 때문이죠. 따라서 모델서버는 서버 식별번호를 부여받아야합니다. 이는 EUREKA 서버에 자신을 등록할 때 메타데이터에 고유번호와 함께 등록시킵니다.